{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SnowNLP快速进行评论数据情感分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SnowNLP 主要可以进行中文分词、词性标注、情感分析、文本分类、转换拼音、繁体转简体、提取文本关键词、提取摘要、分割句子、文本相似等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snownlp import SnowNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试一条京东的好评数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.999950702449061"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SnowNLP(\"本本已收到，体验还是很好，功能方面我不了解，只看外观还是很不错很薄，很轻，也有质感。\").sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 中评数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03251402883400323"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SnowNLP(\"屏幕分辨率一般，送了个极丑的鼠标。\").sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 差评数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0036849517156107847"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SnowNLP(\"很差的一次购物体验，细节做得极差了，还有发热有点严重啊，散热不行，用起来就是烫得厉害，很垃圾！！！\").sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自定义模型训练和保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snownlp import sentiment\n",
    "\n",
    "sentiment.train(\"data/neg.txt\", \"data/pos.txt\")\n",
    "sentiment.save(\"sentiment.marshal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 好评"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6089407099697889"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment.classify(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 差评"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.271552418168417"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment.classify(\"标准间太差房间还不如3星的而且设施非常陈旧.建议酒店把老的标准间从新改善.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于标注好的情感词典来计算情感值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载玻森情感词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>最尼玛</td>\n",
       "      <td>-6.704000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>扰民</td>\n",
       "      <td>-6.497564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fuck...</td>\n",
       "      <td>-6.329634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RNM</td>\n",
       "      <td>-6.218613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wcnmlgb</td>\n",
       "      <td>-5.967100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       key     score\n",
       "0      最尼玛 -6.704000\n",
       "1       扰民 -6.497564\n",
       "2  fuck... -6.329634\n",
       "3      RNM -6.218613\n",
       "4  wcnmlgb -5.967100"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_table(\"data/BosonNLP_sentiment_score.txt\", sep=\" \", names=[\"key\", \"score\"])\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "key = df[\"key\"].values.tolist()\n",
    "score = df[\"score\"].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结巴分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getscore(line):\n",
    "    segs = jieba.lcut(line)  #分词\n",
    "    score_list  = [score[key.index(x)] for x in segs if(x in key)]\n",
    "    return  sum(score_list)  #计算得分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获得句子得分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.26"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line = \"今天天气很好，我很开心\"\n",
    "round(getscore(line),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.96"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line = \"今天下雨，心情也受到影响。\"\n",
    "round(getscore(line),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 绘制情感树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 股吧数据情感分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import random\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "from keras.models import model_from_json\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 中文语料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopwords = pd.read_csv(\"data/stopwords.txt\",index_col=False, quoting=3, sep=\"\\t\", names=['stopword'], encoding='utf-8')\n",
    "stopwords = stopwords['stopword'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>title</th>\n",
       "      <th>time</th>\n",
       "      <th>content</th>\n",
       "      <th>replay</th>\n",
       "      <th>all_replay</th>\n",
       "      <th>time.1</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>334</td>\n",
       "      <td>蝴蝶效应啊，国家队就不能伸出援手吗</td>\n",
       "      <td>2018/6/22 9:34</td>\n",
       "      <td>蝴蝶效应啊，国家队就不能伸出援手吗</td>\n",
       "      <td>（2）</td>\n",
       "      <td>\\r\\n                            救不起呀\\r\\n      ...</td>\n",
       "      <td>2018-06-22  09:45:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>341</td>\n",
       "      <td>根据港股跌幅计算，中兴通讯今天明天必开板。</td>\n",
       "      <td>2018/6/22 9:42</td>\n",
       "      <td>根据港股跌幅计算，中兴通讯今天明天必开板。</td>\n",
       "      <td>（3）</td>\n",
       "      <td>\\r\\n                            看看还有多少封单，别忽悠了，...</td>\n",
       "      <td>2018-06-22  09:47:31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>344</td>\n",
       "      <td>博傻开始</td>\n",
       "      <td>2018/6/22 9:35</td>\n",
       "      <td>今天半仓，明天低开全仓</td>\n",
       "      <td>（25）</td>\n",
       "      <td>\\r\\n                            今天打不开吧？这么大的压单\\...</td>\n",
       "      <td>2018-06-22  09:40:28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>345</td>\n",
       "      <td>窒息</td>\n",
       "      <td>2018/6/22 11:58</td>\n",
       "      <td>110万资金惨遭七连跌的杀戮，只剩下51万，赤裸裸的屠杀</td>\n",
       "      <td>（2）</td>\n",
       "      <td>\\r\\n                            能剩1万算你牛！\\r\\n  ...</td>\n",
       "      <td>2018-06-22  15:38:35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>346</td>\n",
       "      <td>亏大了一一一被平仓一一一也平不掉</td>\n",
       "      <td>2018/6/22 12:46</td>\n",
       "      <td>000063：5万自己的加5万荣资的，现在都是卷商的还卖不掉，还在吹我加钱吖</td>\n",
       "      <td>（3）</td>\n",
       "      <td>\\r\\n                            偷鸡不成蚀把米啊，你想着赚更...</td>\n",
       "      <td>2018-06-22  12:59:25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Id                  title             time  \\\n",
       "0  334      蝴蝶效应啊，国家队就不能伸出援手吗   2018/6/22 9:34   \n",
       "1  341  根据港股跌幅计算，中兴通讯今天明天必开板。   2018/6/22 9:42   \n",
       "2  344                   博傻开始   2018/6/22 9:35   \n",
       "3  345                     窒息  2018/6/22 11:58   \n",
       "4  346       亏大了一一一被平仓一一一也平不掉  2018/6/22 12:46   \n",
       "\n",
       "                                  content replay  \\\n",
       "0                       蝴蝶效应啊，国家队就不能伸出援手吗    （2）   \n",
       "1                   根据港股跌幅计算，中兴通讯今天明天必开板。    （3）   \n",
       "2                             今天半仓，明天低开全仓   （25）   \n",
       "3            110万资金惨遭七连跌的杀戮，只剩下51万，赤裸裸的屠杀    （2）   \n",
       "4  000063：5万自己的加5万荣资的，现在都是卷商的还卖不掉，还在吹我加钱吖    （3）   \n",
       "\n",
       "                                          all_replay                time.1  \\\n",
       "0  \\r\\n                            救不起呀\\r\\n      ...  2018-06-22  09:45:00   \n",
       "1  \\r\\n                            看看还有多少封单，别忽悠了，...  2018-06-22  09:47:31   \n",
       "2  \\r\\n                            今天打不开吧？这么大的压单\\...  2018-06-22  09:40:28   \n",
       "3  \\r\\n                            能剩1万算你牛！\\r\\n  ...  2018-06-22  15:38:35   \n",
       "4  \\r\\n                            偷鸡不成蚀把米啊，你想着赚更...  2018-06-22  12:59:25   \n",
       "\n",
       "   label  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data1 = pd.read_csv(\"data/data1.csv\", encoding='utf-8')\n",
    "df_data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#把内容有缺失值的删除\n",
    "df_data1.dropna(inplace=True)\n",
    "\n",
    "#抽取文本数据和标签\n",
    "data_1 = df_data1.loc[:,['content', 'label']]\n",
    "\n",
    "#把消极  中性  积极分别为0、1、2的预料分别拿出来\n",
    "data_label_0 = data_1.loc[data_1['label'] ==0, :]\n",
    "data_label_1 = data_1.loc[data_1['label'] ==1, :]\n",
    "data_label_2 = data_1.loc[data_1['label'] ==2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>蝴蝶效应啊，国家队就不能伸出援手吗</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>根据港股跌幅计算，中兴通讯今天明天必开板。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>今天半仓，明天低开全仓</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>110万资金惨遭七连跌的杀戮，只剩下51万，赤裸裸的屠杀</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000063：5万自己的加5万荣资的，现在都是卷商的还卖不掉，还在吹我加钱吖</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  content  label\n",
       "0                       蝴蝶效应啊，国家队就不能伸出援手吗      0\n",
       "1                   根据港股跌幅计算，中兴通讯今天明天必开板。      0\n",
       "2                             今天半仓，明天低开全仓      0\n",
       "3            110万资金惨遭七连跌的杀戮，只剩下51万，赤裸裸的屠杀      0\n",
       "4  000063：5万自己的加5万荣资的，现在都是卷商的还卖不掉，还在吹我加钱吖      0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_label_0[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#定义分词函数\n",
    "def preprocess_text(content_lines, sentences, category):\n",
    "    for line in content_lines:\n",
    "        try:\n",
    "            segs=jieba.lcut(line)\n",
    "            segs = filter(lambda x:len(x)>1, segs)\n",
    "            segs = [v for v in segs if not str(v).isdigit()]#去数字\n",
    "            segs = list(filter(lambda x:x.strip(), segs)) #去左右空格\n",
    "            segs = filter(lambda x:x not in stopwords, segs)\n",
    "            temp = \" \".join(segs)\n",
    "            if(len(temp)>1):\n",
    "                sentences.append((temp, category))\n",
    "        except Exception:\n",
    "            print(line)\n",
    "            continue "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 复杂规则"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.882 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "#获取数据\n",
    "data_label_0_content = data_label_0['content'].values.tolist()\n",
    "data_label_1_content = data_label_1['content'].values.tolist()\n",
    "data_label_2_content = data_label_2['content'].values.tolist()\n",
    "\n",
    "#生成训练数据\n",
    "sentences = []\n",
    "preprocess_text(data_label_0_content, sentences, 0)\n",
    "preprocess_text(data_label_1_content, sentences, 1)\n",
    "preprocess_text(data_label_2_content, sentences, 2)\n",
    "\n",
    "#我们打乱一下顺序，生成更可靠的训练集\n",
    "random.shuffle(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#所以把原数据集分成训练集的测试集，咱们用sklearn自带的分割函数。\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x, y = zip(*sentences)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3,random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=4000, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#抽取特征，我们对文本抽取词袋模型特征\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer(\n",
    "    analyzer='word', #tokenise by character ngrams\n",
    "    max_features=4000,  #keep the most common 1000 ngrams\n",
    ")\n",
    "vec.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 算法建模"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 设置参数\n",
    "max_features = 5001\n",
    "maxlen = 100\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 10\n",
    "nclasses = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 转成数组和标签处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (1912, 100)\n",
      "x_test shape: (820, 100)\n"
     ]
    }
   ],
   "source": [
    "x_train = vec.transform(x_train)\n",
    "x_test = vec.transform(x_test)\n",
    "x_train = x_train.toarray()\n",
    "x_test = x_test.toarray()\n",
    "y_train = np_utils.to_categorical(y_train, nclasses)\n",
    "y_test = np_utils.to_categorical(y_test, nclasses)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义一个绘制 Loss 曲线的类："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = {'batch':[], 'epoch':[]}\n",
    "        self.accuracy = {'batch':[], 'epoch':[]}\n",
    "        self.val_loss = {'batch':[], 'epoch':[]}\n",
    "        self.val_acc = {'batch':[], 'epoch':[]}\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses['batch'].append(logs.get('loss'))\n",
    "        self.accuracy['batch'].append(logs.get('acc'))\n",
    "        self.val_loss['batch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['batch'].append(logs.get('val_acc'))\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses['epoch'].append(logs.get('loss'))\n",
    "        self.accuracy['epoch'].append(logs.get('acc'))\n",
    "        self.val_loss['epoch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['epoch'].append(logs.get('val_acc'))\n",
    "\n",
    "    def loss_plot(self, loss_type):\n",
    "        iters = range(len(self.losses[loss_type]))\n",
    "        plt.figure()\n",
    "        # acc\n",
    "        plt.plot(iters, self.accuracy[loss_type], 'r', label='train acc')\n",
    "        # loss\n",
    "        plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n",
    "        if loss_type == 'epoch':\n",
    "            # val_acc\n",
    "            plt.plot(iters, self.val_acc[loss_type], 'b', label='val acc')\n",
    "            # val_loss\n",
    "            plt.plot(iters, self.val_loss[loss_type], 'k', label='val loss')\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(loss_type)\n",
    "        plt.ylabel('acc-loss')\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Train on 1912 samples, validate on 820 samples\n",
      "Epoch 1/10\n",
      "1912/1912 [==============================] - 2s 1ms/step - loss: 0.4041 - acc: 0.9111 - val_loss: 0.4523 - val_acc: 0.9037\n",
      "Epoch 2/10\n",
      "1912/1912 [==============================] - 2s 1ms/step - loss: 0.3503 - acc: 0.9168 - val_loss: 0.4307 - val_acc: 0.9037\n",
      "Epoch 3/10\n",
      "1912/1912 [==============================] - 2s 1ms/step - loss: 0.3535 - acc: 0.9168 - val_loss: 0.5107 - val_acc: 0.9037\n",
      "Epoch 4/10\n",
      "1912/1912 [==============================] - 2s 1ms/step - loss: 0.3461 - acc: 0.9168 - val_loss: 0.5179 - val_acc: 0.9037\n",
      "Epoch 5/10\n",
      "1912/1912 [==============================] - 2s 1ms/step - loss: 0.3577 - acc: 0.9168 - val_loss: 0.4246 - val_acc: 0.9037\n",
      "Epoch 6/10\n",
      "1912/1912 [==============================] - 2s 1ms/step - loss: 0.3544 - acc: 0.9168 - val_loss: 0.4873 - val_acc: 0.9037\n",
      "Epoch 7/10\n",
      "1912/1912 [==============================] - 2s 1ms/step - loss: 0.3494 - acc: 0.9168 - val_loss: 0.4521 - val_acc: 0.9037\n",
      "Epoch 8/10\n",
      "1912/1912 [==============================] - 2s 1ms/step - loss: 0.3474 - acc: 0.9168 - val_loss: 0.5042 - val_acc: 0.9037\n",
      "Epoch 9/10\n",
      "1912/1912 [==============================] - 2s 1ms/step - loss: 0.3544 - acc: 0.9168 - val_loss: 0.4635 - val_acc: 0.9037\n",
      "Epoch 10/10\n",
      "1912/1912 [==============================] - 2s 1ms/step - loss: 0.3536 - acc: 0.9168 - val_loss: 0.4839 - val_acc: 0.9037\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17fd0b00>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = LossHistory()\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(max_features,\n",
    "                        embedding_dims,\n",
    "                        input_length=maxlen))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(filters,\n",
    "                     kernel_size,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nclasses))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test), callbacks=[history])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 情感分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Anaconda3]",
   "language": "python",
   "name": "Python [Anaconda3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
